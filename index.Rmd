---
title: "Confidence Intervals for Variances from lm"
author: "Kevin"
date: "Friday, April 03, 2015"
output: 
  html_document:
    theme: cerulean
---

<a href="https://github.com/kferris10/CIs-for-lm-variances"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/365986a132ccd6a44c23a9169022c0b5c890c387/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f7265645f6161303030302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"></a>

```{r knitr-setup, echo=FALSE, warning=FALSE, message=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE, 
                      tidy = FALSE,
                      fig.height = 4, 
                      fig.width = 6, 
                      fig.align = "center", 
                      size = "fotenotesize", 
                      digits = 3, 
                      comment = "#>")
options(show.signif.stars = F, 
        digits = 3)
library(dplyr)
```


## Very Quick Example

Simulated 95% confidence interval for the residual standard error from an `lm` model for the mtcars dataset.

```{r mtcars_sim}
m1 <- lm(mpg ~ wt, data = mtcars)
library(arm)
sims <- sim(m1, 1000)
quantile(sims@sigma, probs = c(.025, .975))
```

## Comparing CIs for Multilevel Models Using `lm`, `lmer`, and `Stan`

This is a very quick comparison of the confidence intervals for variance components.  I put it up at the top so you we can see the results without having to follow along with all of my code.  I used `lm`, `lmer`, and `Stan` to estimate the variability at each level in the sampling hierarchy and here are 95% confidence intervals using each method.

```{r comparison_table, echo=FALSE, results='asis'}
data_frame(
  Level = c("Core", "Tree", "Plot", "Site"), 
  Truth = c(.830, .286, .937, .915), 
  lm = c("(.627, .921)", "(.487, .839)", "(1.06, 3.63)", "(.518, 2.36)"), 
  lmer = c("(.628, .874)", "(.000, .404)", "(.891, 2.125)", "(.000, 1.258)"), 
  stan = c("(.63, .87)", "(.01, .45)", "(.99, 2.97)", "(.04, 3.75)")
  ) %>% 
  knitr::kable(align = "l")
```

The `Stan` and `lmer` results seem to be fairly similar and reasonably accurate.  It does appear as though `Stan` is a little bit more conservative though.  The `lm` results are worrisome though as the confidence intervals for tree and plot level variability are off by quite a bit.


## The Linear Model

The following linear model can be fit in R with the `lm` function.

$mpg_i = \beta_{0} + \beta_{1} wt_{i} + \epsilon_{i}$

$\epsilon_i \sim N(0, \sigma^2)$

```{r mtcars_lm}
m2 <- lm(mpg ~ wt, data = mtcars)
summary(m2)
```

In this model, there are three parameters: $\beta_0$, $\beta_1$, and $\sigma$.  R estimates these parameters as 37.3, -5.3, and 3.0 respectively.  

## The `sim` function

The `arm` package was written by Andrew Gelman and Yu-Song Su to complement Gelman's *Data Analysis Using Regression and Multilevel/Hierarchical
Models* book.  In Chapter 7 of this book, they discuss how to use simulation for inferences in regression context.

To aid with these inferences, they provided the `sim` function in the package.  This function simulates estimated parameters from the fitted model.  In the code below, I simulate $\beta_0$, $\beta_1$, and $\sigma$ 1000 times.  The `str` command reveals the structure of an R object.  We can see that the simulated values are stored in two separate parts of the `sims` object.  The estimated regression coefficients ($\beta_0$ and $\beta_1$) are stored in a matrix named `coef`, and the estimated variabilities (just $\sigma$ in this example) are named `sigma`.

```{r lm_sims}
library(arm)
sims <- sim(m2, 1000)
str(sims)
```

## Confidence Intervals for $\sigma$

These simulations can be used to provide a summary of the model's suggested values of the parameters.

```{r sims_summary}
summary(sims@sigma) ## the @ sign is used because sim uses S4 class objects
```

We can visualize the distribution of simulated values.

```{r sims_density}
library(ggplot2)
qplot(sims@sigma, geom = "density", fill = I("lightblue")) + 
  labs(x = expression(sigma), 
       title = "Simulated Distribution of Residual Standard Error")
```

To generate a confidence interval for $\sigma$, we can just extract the middle 95% of this distribution using the `quantile` function.

```{r sims_ci}
quantile(sims@sigma, probs = c(.025, .975))
```

## CIs for Multilevel Models Using `lm`

```{r sim_tree_data, echo=FALSE}
library(dplyr)
set.seed(42)

# variability at each level
sigma_site <- runif(1, 0, 1)
sigma_plot <- runif(1, 0, 1)
sigma_tree <- runif(1, 0, 1)  
sigma_core <- runif(1, 0, 1)

# LEF   ------------------------------------------------

# number of observations at each level
n_sites <- 4
n_plots <- 2
n_trees <- 6
n_cores <- 1
n_obs <- prod(n_sites, n_plots, n_trees, n_cores)

# simulating data
lef <- data_frame(
  location = "LEF", 
  site = paste0("S", rep(1:n_sites, each = n_obs / n_sites)), 
  plot = paste0("P", rep(rep(1:n_plots, each = n_obs / (n_sites * n_plots)), 
                         times = n_sites)), 
  tree = paste0("T", rep(rep(1:n_trees, each = n_obs / (n_sites * n_plots * n_trees)), 
                         times = n_sites * n_plots)), 
  core = paste0("C", rep(rep(1:n_cores, each = n_obs / (n_sites * n_plots * n_trees * n_cores)))), 
  species = ifelse(substr(tree, 2, 2) <= 3, "DF", 
                   ifelse(site == "S1", "WL", 
                          ifelse(site == "S2", "PP", 
                                 ifelse(site == "S3", "EB", 
                                        "DR")))), 
  elevation = ifelse(site %in% c("S1", "S2"), "H", "L"), 
  aspect = ifelse(site %in% c("S1", "S3"), "N", "S"), 
  topo = ifelse(plot == "P1", "valley", "slope")
) %>% 
  mutate(int = rnorm(1, 3, .5)) %>% 
  group_by(site) %>% 
  mutate(b_site = rnorm(1, 0, sigma_site)) %>% 
  group_by(site, plot) %>% 
  mutate(b_plot = rnorm(1, 0, sigma_plot)) %>% 
  group_by(site, plot, tree) %>% 
  mutate(b_tree = rnorm(1, 0, sigma_tree)) %>% 
  group_by(site, plot, tree, core) %>% 
  mutate(b_core = rnorm(1, 0, sigma_core)) %>% 
  ungroup() %>% 
  mutate(m_c = int + b_site + b_plot + b_tree + b_core)

# BZN ---------------------------------------------------

n_sites <- 2
n_plots <- 2
n_trees <- 4
n_cores <- 4
n_obs <- prod(n_sites, n_plots, n_trees, n_cores)

bzn <- data_frame(
  location = "BZN", 
  site = paste0("S", rep(1:n_sites, each = n_obs / n_sites)), 
  plot = paste0("P", rep(rep(1:n_plots, each = n_obs / (n_sites * n_plots)), 
                         times = n_sites)), 
  tree = paste0("T", rep(rep(1:n_trees, each = n_obs / (n_sites * n_plots * n_trees)), 
                         times = n_sites * n_plots)), 
  core = paste0("C", rep(rep(1:n_cores, each = n_obs / (n_sites * n_plots * n_trees * n_cores)), 
                         times = n_sites * n_plots * n_trees)), 
  species = "DF", 
  elevation = ifelse(site %in% c("S1", "S2"), "H", "L"), 
  aspect = ifelse(site %in% c("S1", "S3"), "N", "S"), 
  canopy = ifelse(plot == "P1", "open", "closed")
)  %>% 
  mutate(int = rnorm(1, 3, .5)) %>% 
  group_by(site) %>% 
  mutate(b_site = rnorm(1, 0, sigma_site)) %>% 
  group_by(site, plot) %>% 
  mutate(b_plot = rnorm(1, 0, sigma_plot)) %>% 
  group_by(site, plot, tree) %>% 
  mutate(b_tree = rnorm(1, 0, sigma_tree)) %>% 
  group_by(site, plot, tree, core) %>% 
  mutate(b_core = rnorm(1, 0, sigma_core)) %>% 
  ungroup() %>% 
  mutate(m_c = int + b_site + b_plot + b_tree + b_core)

# for each level ------------------------------------------
tree_data <- bind_rows(bzn, lef) %>% 
  filter(species == "DF")            ## only Douglas Firs

# core level
by_core <- tree_data %>% 
  group_by(location, site, plot, tree, core) %>% 
  summarise(m_c = mean(m_c)) %>% 
  ungroup() %>% 
  mutate(site = interaction(location, site), 
         plot = interaction(location, site, plot), 
         tree = interaction(location, site, plot, tree), 
         core = interaction(location, site, plot, tree))
# tree level
by_tree <- tree_data %>% 
  group_by(location, site, plot, tree) %>% 
  summarise(m_c = mean(m_c)) %>% 
  ungroup() %>% 
  mutate(site = interaction(location, site), 
         plot = interaction(location, site, plot), 
         tree = interaction(location, site, plot, tree))
# plot level
by_plot <- tree_data %>% 
  group_by(location, site, plot) %>% 
  summarise(m_c = mean(m_c)) %>% 
  ungroup() %>% 
  mutate(site = interaction(location, site), 
         plot = interaction(location, site, plot))
# site level
by_site <- tree_data %>% 
  group_by(location, site) %>% 
  summarise(m_c = mean(m_c)) %>% 
  ungroup() %>% 
  mutate(site = interaction(location, site))
```

Estimated 95% confidence intervals

```{r mods_cis}
# models by level
mod_core <- lm(m_c ~ tree, data = by_core)
mod_tree <- lm(m_c ~ plot, data = by_tree)
mod_plot <- lm(m_c ~ site, data = by_plot)
mod_site <- lm(m_c ~ location, data = by_site)
# simulating RSE at each level
sim_sigmas <- sapply(list(mod_core, mod_tree, mod_plot, mod_site), function(x) {
  sim(x, 500)@sigma
})
# calculating CIs
apply(sim_sigmas, 2, quantile, probs = c(.025, .975)) %>% 
  data.frame() %>% 
  setNames(c("CoreLevel", "TreeLevel", "PlotLevel", "SiteLevel"))
```

Comparing using `ggvis`

```{r mods_densities}
# turn simulations into long format
library(tidyr)  ## to go to long format
sim_sigmas_df <- sim_sigmas %>% 
  data.frame() %>%   ## have to for tidyr
  setNames(c("core_sd", "tree_sd", "plot_sd", "site_sd")) %>% 
  gather(level, sigma)  ## going to long format
# density plots
library(ggvis)
sim_sigmas_df %>% 
  ggvis(~sigma, fill = ~level) %>% 
  group_by(level) %>% 
  layer_densities()
```

Comparing to an `lmer` model

```{r lmer_model}
library(lme4)
m3 <- lmer(m_c ~ location + (1 | site/plot/tree), data = by_core)
confint(m3)
```

## Bayesian Hierarchical Model with Stan

Here's the model written and fit in Stan.  I'm omitting all the Stan messages to save space.

```{r stan_mod, results='hide'}
stan_mod <- "
data {
  int N;  
  int J;  
  int K;  
  int L;  
  
  vector[N] y;
  int<lower=0, upper=1> locLEF[N];
  int<lower=1, upper=J> site[N];
  int<lower=1, upper=K> plot[N];
  int<lower=1, upper=L> tree[N];
}
parameters {
  real beta0;
  real beta1;
  real<lower=0, upper=10> sig_site;
  real<lower=0, upper=10> sig_plot;
  real<lower=0, upper=10> sig_tree;
  real<lower=0, upper=10> sig_core;
  
  vector[J] b_site;
  vector[K] b_plot;
  vector[L] b_tree;
}
transformed parameters {
  vector[N] yhat;
  
  for(i in 1:N) 
    yhat[i] <- beta0 + beta1 * locLEF[i] + 
    b_site[site[i]] +  b_plot[plot[i]] + b_tree[tree[i]];
}
model {
  y ~ normal(yhat, sig_core);
  beta0 ~ normal(0, 100);
  beta1 ~ normal(0, 100);
  b_site ~ normal(0, sig_site);
  b_plot ~ normal(0, sig_plot);
  b_tree ~ normal(0, sig_tree);
}
"
library(rstan)
trans <- stanc(model_code = stan_mod)
comp <- stan_model(stanc_ret = trans)

# data for stan
dat_stan <- with(by_core %>% droplevels(), 
                 list(N = nrow(by_core), 
                      J = n_distinct(site), 
                      K = n_distinct(plot), 
                      L = n_distinct(tree), 
                      y = m_c, 
                      locLEF = ifelse(location == "LEF", 1, 0), 
                      site = as.integer(as.factor(site)), 
                      plot = as.integer(as.factor(plot)), 
                      tree = as.integer(as.factor(tree))
                      )
                 )
inits <- list(beta0 = 1, 
       beta1 = 1, 
       b_site = rep(1, 6), 
       b_plot = rep(1, 12), 
       b_tree = rep(1, 40), 
       sig_site = sigma_site, 
       sig_plot = sigma_plot, 
       sig_tree = sigma_tree, 
       sig_core = sigma_core)
# sampling parameters
samps <- sampling(comp, data = dat_stan, chains = 4, iter = 2000, thin = 8, 
                  init = list(inits, inits, inits, inits))
```

Stan is having a really tough time identifying $\sigma_{tree}$.  I'm guessing this is because it is so small relative to the others.  

```{r stan_results, fig.width=6, fig.height=6}
print(samps, pars = c("sig_site", "sig_plot", "sig_tree", "sig_core"))
traceplot(samps, pars = c("sig_site", "sig_plot", "sig_tree", "sig_core"), inc_warmup = F)
```

Stan gives us generally the same results as `lmer`.  The major difference is is that Stan has a much wider intervals for the variability between sites.  I have no idea why this is.

## Code to Simulate Data

```{r sim_tree_data, echo=TRUE, eval=FALSE}
```




